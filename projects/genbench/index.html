<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8" />
		<!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
		<!-- Replace the content tag with appropriate information -->
		<meta name="description" content="DESCRIPTION META TAG" />
		<meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
		<meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
		<meta property="og:url" content="URL OF THE WEBSITE" />
		<!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
		<meta property="og:image" content="static/image/your_banner_image.png" />
		<meta property="og:image:width" content="1200" />
		<meta property="og:image:height" content="630" />

		<meta name="twitter:title" content="TWITTER BANNER TITLE META TAG" />
		<meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG" />
		<!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
		<!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png" /> -->
		<meta name="twitter:card" content="summary_large_image" />
		<!-- Keywords for your paper to be indexed by-->
		<meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />

		<title>Benchmarking and Analyzing Generative Data for Visual Recognition</title>
		<!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico" /> -->
		<link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />

		<link rel="stylesheet" href="static/css/bulma.min.css" />
		<link rel="stylesheet" href="static/css/bulma-carousel.min.css" />
		<link rel="stylesheet" href="static/css/bulma-slider.min.css" />
		<link rel="stylesheet" href="static/css/fontawesome.all.min.css" />
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
		<link rel="stylesheet" href="static/css/index.css" />

		<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
		<script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
		<script defer src="static/js/fontawesome.all.min.js"></script>
		<script src="static/js/bulma-carousel.min.js"></script>
		<script src="static/js/bulma-slider.min.js"></script>
		<script src="static/js/index.js"></script>
	</head>
	<body>
		<section class="hero">
			<div class="hero-body">
				<div class="container is-max-desktop">
					<div class="columns is-centered">
						<div class="column has-text-centered">
							<h1 class="title is-1 publication-title">Benchmarking and Analyzing Generative Data for Visual Recognition</h1>
							<div class="is-size-5 publication-authors">
								<!-- Paper authors -->
								<span class="author-block">
									<a href="https://brianboli.com/" target="_blank">Bo Li<sup>1</sup></a>,</span>
								<span class="author-block">
									<a href="https://hliu.cc/" target="_blank">Haotian Liu<sup>2</sup></a>,</span>
								<span class="author-block"><a href="https://cliangyu.com/" target="_blank">Liangyu Chen<sup>1</sup></a></span>
								<span class="author-block"><a href="https://pages.cs.wisc.edu/~yongjaelee/" target="_blank">Yong Jae Lee<sup>2</sup></a></span>
								<span class="author-block"><a href="https://chunyuan.li/" target="_blank">Chunyuan Li<sup>3*</sup></a></span>
								<span class="author-block">
									<a href="https://liuziwei7.github.io/" target="_blank">Ziwei Liu<sup>1*,✉️</sup></a>
								</span>
							</div>

							<div class="is-size-5 publication-authors">
								<span class="author-block"><sup>1</sup>S-Lab, Nanyang Technological University, Singapore</span><br>
								<span class="author-block"><sup>2</sup>University of Wisconsin-Madison, USA</span><br>
								<span class="author-block"><sup>3</sup>Microsoft Research, USA</span>
								<span class="eql-cntrb"><small><br /><sup>*</sup>Indicates Equal Advisory Contribution</small></span>
                                <span class="eql-cntrb"><small><br /><sup>✉️</sup>Corresponding Author</small></span>
							</div>

							<div class="column has-text-centered">
								<div class="publication-links">
									<!-- Arxiv PDF link -->
									<span class="link-block">
										<a
											href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf"
											target="_blank"
											class="external-link button is-normal is-rounded is-dark"
										>
											<span class="icon">
												<i class="fas fa-file-pdf"></i>
											</span>
											<span>Paper</span>
										</a>
									</span>

									<!-- Supplementary PDF link -->
									<!-- <span class="link-block">
										<a
											href="static/pdfs/supplementary_material.pdf"
											target="_blank"
											class="external-link button is-normal is-rounded is-dark"
										>
											<span class="icon">
												<i class="fas fa-file-pdf"></i>
											</span>
											<span>Supplementary</span>
										</a>
									</span> -->

									<!-- Github link -->
									<span class="link-block">
										<a
											href="https://github.com/Luodian/Genbench"
											target="_blank"
											class="external-link button is-normal is-rounded is-dark"
										>
											<span class="icon">
												<i class="fab fa-github"></i>
											</span>
											<span>Code</span>
										</a>
									</span>

									<!-- ArXiv abstract Link -->
									<span class="link-block">
										<a
											href="https://arxiv.org/abs/<ARXIV PAPER ID>"
											target="_blank"
											class="external-link button is-normal is-rounded is-dark"
										>
											<span class="icon">
												<i class="ai ai-arxiv"></i>
											</span>
											<span>arXiv</span>
										</a>
									</span>
								</div>
							</div>
						</div>
					</div>
				</div>
			</div>
		</section>

		<!-- Teaser video-->
		<section class="hero teaser">
			<div class="container is-max-desktop">
				<div class="hero-body">
					<!-- <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
						<!-- Your video here -->
						<!-- <source src="static/videos/banner_video.mp4" type="video/mp4" /> -->
					<!-- </video> -->
					<h2 class="subtitle">
                        <strong>Motivation:</strong> machine learning (ML) community has successfully utilized datasets, though their creation often demands substantial time and resources. Despite this, there's a lack of efficient, open data engineering tools to streamline these processes, leading to increased costs. 
                        This prompts an exploration into the potential benefits of generative data, a topic investigated in this paper.
					</h2>
				</div>
			</div>
		</section>
		<!-- End teaser video -->

		<!-- Paper abstract -->
		<section class="section hero is-light">
			<div class="container is-max-desktop">
				<div class="columns is-centered has-text-centered">
					<div class="column is-four-fifths">
						<h2 class="title is-3">Abstract</h2>
						<div class="content has-text-justified">
							<p>
                                Large pre-trained generative models have made remarkable progress in generating realistic and diverse images. This ability opens up the possibility of utilizing large pre-trained generative models as efficient data generators to enhance visual recognition. In this work, we aim to rigorously benchmark and analyze the effect of generative images, with an emphasis on comparative studies among different paradigms leveraging external data (i.e. generative vs. retrieval vs. original). 
                                Our contributions include:
                                <br>
                                <strong>(1) Benchmarking with wide coverage:</strong> We construct \textbf{GenBench}, a comprehensive benchmark consisting of 22 datasets with 2548 categories to cover a diverse range of visual recognition tasks to evaluate the benefits of generative data. 
                                <br>
                                <strong>(2) New metric tailored for recognition:</strong> Existing metrics in generative models (\eg, FID and CLIP score) have no strong correlation for the downstream recognition performance. Here we propose a training-free metric, \textbf{CLER score}, to efficiently indicate the effectiveness of generative data for recognition performance before actually training on the downstream tasks. 
                                <br>
                                <strong>(3) New reference baselines:</strong> Leveraging external training data includes retrieval-based methods. We compare generative data with retrieved data from the same external pool to highlight the unique characteristics of generative data.
                                <br>
                                <strong>(4) Investigating external knowledge injection in generative models:</strong> We fine-tune special token embeddings for each category in a dataset by injecting retrieval and original data via Textual Inversion. This approach leads to improved performance across 17 datasets, with a notable exception in the case of low-resolution reference images.
                                <br>
                                Overall, our comprehensive benchmark, in-depth analysis, and proposed techniques highlight the opportunities of generative data for visual recognition, while also identifying critical challenges for future research.
							</p>
						</div>
					</div>
				</div>
			</div>
		</section>

        <section class="hero is-small is-light">
			<div class="hero-body">
				<div class="container">
					<h2 class="title">Figures/Tables</h2>
                    <div class="image-container">
                    <!-- <iframe src="static/images/rel_improvement.png" width="100%" height="100%"> </iframe> -->
                    <div style="background-color: white;">
                        <figure>
                            <div class="has-text-centered">
                            <img src="static/images/rel_improvement.png" alt="Image description" width="90%">
                            </div>
                            <figure class="has-text-centered">
                                <figcaption style="font-size: 1.2em;"><strong>Table: Left:</strong> CLIP ViT-B/32 linear probing results for all datasets on GenBench, arranged in descending order of improvement over the zero-shot accuracy. The results are based on 500-shot generative data per category with best strategy for each dataset. <br><strong>Right:</strong> The average results using different external data sources on the 22 datasets, along with sample images for different categories, are shown on the right.</figcaption>
                            </figure>
                        </figure>
                    </div>
                    <br>
                    <div style="background-color: white;">
                        <figure>
                            <div class="has-text-centered">
                                <img src="static/images/metric_correlation.png" alt="Image description" width="50%">
                            </div>
                            <figure class="has-text-centered">
                                <figcaption style="font-size: 1.2em;"><strong>Figure:</strong> Correlation between three evaluation metrics, CLER score, CLIP score, and FID, with the linear probing accuracy.</figcaption>
                            </figure>
                        </figure>
                    </div>
                    <br>
                    <div style="background-color: white;">
                        <figure>
                            <div class="has-text-centered">
                                <img src="static/images/appendix_examples.png" alt="Image description" width="50%">
                            </div>
                            <figure class="has-text-centered">
                                <figcaption style="font-size: 1.2em;"><strong>Figure:</strong> Generative images with different prompt strategies.</figcaption>
                            </figure>
                        </figure>
                    </div>
                    <br>
                    <div style="background-color: white;">
                        <figure>
                            <div class="has-text-centered">
                                <img src="static/images/ti_teaser.png" alt="Image description" width="50%">
                            </div>
                            <figure class="has-text-centered">
                                <figcaption style="font-size: 1.2em;"><strong>Figure:</strong> Comparison of the visualization results between direct sampling from Stable Diffusion and finetuned after Textual Inversion~\cite{gal2022image} on original and retrieval data. </figcaption>
                            </figure>
                        </figure>
                    </div>
                    </div>
                    </div>
                    </section>
                    </body>
					<!-- <iframe src="static/images/rel_improvement.png" width="100%" height="100%"> </iframe> -->
				</div>
			</div>
		</section>
		<!-- End paper abstract -->
		<!-- End image carousel -->
		<!-- Paper poster -->
		<!--End paper poster -->

		<!--BibTex citation -->
		<section class="section" id="BibTeX">
			<div class="container is-max-desktop content">
				<h2 class="title">BibTeX</h2>
				<pre><code>BibTex Code Here</code></pre>
			</div>
		</section>
		<!--End BibTex citation -->

		<!-- Statcounter tracking code -->

		<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

		<!-- End of Statcounter Code -->
	</body>
</html>
