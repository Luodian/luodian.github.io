<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Brian (Bo) Li</title>
<meta name="description" content="">

<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.17.0/css/mdb.min.css" integrity="sha256-/SwJ2GDcEt5382i8zqDwl36VJGECxEoIcBIuoLmLR4g=" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css"  integrity="sha256-h20CPZ0QyXlBuAw7A+KluUYx/3pK+c7lYEpqLTlxjYQ=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">
<link rel="stylesheet" media="screen" href="https://fontlibrary.org//face/carlito" type="text/css"/> 
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Nunito:ital,wght@0,300;0,400;0,500;0,600;1,400;1,500;1,600&display=swap" rel="stylesheet"> 

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/">

<!-- Open Graph -->


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light bg-white navbar-expand-sm fixed-top">
    <div class="container">
      
      
      
      
      
      <a class="navbar-brand title font-weight-lighter" href="http://brianboli.com">
       <span class="font-weight-bold">Brian Li</span>
      </a>
      
      
        <!-- Social Icons -->
        <div class="row ml-1 ml-sm-0">
          <span class="contact-icon">
  <!-- <a href="mailto:"><i class="fas fa-envelope"></i></a> -->
  
  <a href="https://scholar.google.com/citations?user=1_zc1-IAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
  
  
  <a href="https://github.com/luodian" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
  <a href="https://www.linkedin.com/in/brianbo1121" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
  <a href="https://twitter.com/BoLi68567011" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>
  
  
  
  
</span>

        </div>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <!-- <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li> -->
          <!-- <li class="nav-item">
            <a class="nav-link" href="//#publications">
            Publications
            </a>
          </li>
          
          <li class="nav-item">
            <a class="nav-link" href="/code/">
            Software
            </a>
          </li> -->

          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    
    
    
    
  </header>

  <article>
    <div class="row">
      <div class="col">
        <p>Ph.D Student, Computer Science <br />
Nanyang Technological University, Singapore <br />
<!-- <a href="assets/pdf/jiaming_cv.pdf" target="_blank"><b>Curriculum Vitae</b></a> --></p>

<hr />
<h4 id="about-me">About Me</h4>

<p>I am a third-year PhD student and luckily advised by <a href="https://liuziwei7.github.io/">Prof. Ziwei Liu</a>. My research focuses on multimodal models and building true intelligence.</p>

<!-- [Feeling the AGI](https://x.com/ilyasut/status/1578238338288402432) and seeing the progress give me a deep passion for what I am doing, driving a commitment not bound by material gain, but by the quiet call of inner beliefs and a true sense of purpose. -->

<p>I am lucky to work with many brilliant researchers in a non-profit research-oriented organization, <a href="https://huggingface.co/lmms-lab">LMMs-Lab</a>, we share the sincere passion for developing multimodal intelligence.</p>

<p><strong>Email</strong>: drluodian[at]gmail[dot]com</p>

<hr />

<h5 id="selected-publications">Selected Publications</h5>

<div class="publications">

<ol class="bibliography"><li><div class="row">
    <div class="col-sm-1">
        [18]
    </div>

    <div id="li2025aero" class="col-sm-11">
        
        <span class="author">
      
      
      
      
      <u>Bo Li*</u>,
      
      
      
      
      
      
      
      
      Change Loy Chen,
      
      
      
      
      
      
      
      
      
      Fanyi Pu,
      
      
      
      
      
      
      
      
      
      Jingkang Yang,
      
      
      
      
      
      
      
      
      
      Kaichen Zhang*,
      
      
      
      
      
      
      
      
      
      Kairui Hu,
      
      
      
      
      
      
      
      
      
      Luu Minh Thang*,
      
      
      
      
      
      
      
      
      
      Nguyen Quang Trung*,
      
      
      
      
      
      
      
      
      
      Pham Ba Cong*,
      
      
      
      
      
      
      
      
      
      Shuai Liu,
      
      
      
      
      
      
      
      
      
      Yezhen Wang*,
      
      
      
      
      
      
      
      
      
      Ziwei Liu
      
      
      
      
      
    </span> 
        <span class="title" style="font-size: 16px;"> <a href="https://www.lmms-lab.com/posts/aero_audio/">Aero-1-Audio</a></span> 

        <span class="periodical">
      <nobr style="color:darkcyan; font-weight: 600;">
        
      </nobr>
      
      
      <em>Technical Blog,</em>
      
      <b style="color:darkred"><br />Open models for wide range of audio tasks, trained on only 50K hours data yet achieving excellent performance, suggesting smart data &gt; massive training..</b>
      
      <!-- 
        2025
       -->

      <nobr class="links" style="font-weight: 600;">
        
        
        
        
        
        
        
        
        
        
      </nobr>

    </span>  


        <!-- Hidden abstract block -->
        
    </div>
</div></li>
<li><div class="row">
    <div class="col-sm-1">
        [17]
    </div>

    <div id="li2024llava" class="col-sm-11">
        
        <span class="author">
      
      
      
      
      <u>Bo Li</u>,
      
      
      
      
      
      
      
      
      Yuanhan Zhang,
      
      
      
      
      
      
      
      
      
      Dong Guo,
      
      
      
      
      
      
      
      
      
      Renrui Zhang,
      
      
      
      
      
      
      
      
      
      Feng Li,
      
      
      
      
      
      
      
      
      
      Hao Zhang,
      
      
      
      
      
      
      
      
      
      Kaichen Zhang,
      
      
      
      
      
      
      
      
      
      Yanwei Li,
      
      
      
      
      
      
      
      
      
      Ziwei Liu,
      
      
      
      
      
      
      
      
      
      Chunyuan Li
      
      
      
      
      
    </span> 
        <span class="title" style="font-size: 16px;"> <a href="https://arxiv.org/abs/2408.03326">LLaVA-OneVision: Easy Visual Task Transfer</a></span> 

        <span class="periodical">
      <nobr style="color:darkcyan; font-weight: 600;">
        
      </nobr>
      
      
      <em>Transactions on Machine Learning Research (TMLR), 2025,</em>
      
      <b style="color:darkred"><br />SOTA-level fully open models (models/data/code)
          achieving GPT-4o-level performance across 30+ image and video
          tasks. Led codebase, datacuration, and evaluation.</b>
      
      <!-- 
        2024
       -->

      <nobr class="links" style="font-weight: 600;">
        
        
        
        
        
        
        
        
        
        
      </nobr>

    </span>  


        <!-- Hidden abstract block -->
        
    </div>
</div></li>
<li><div class="row">
    <div class="col-sm-1">
        [16]
    </div>

    <div id="li2024lmmseval" class="col-sm-11">
        
        <span class="author">
      
      
      
      
      
      Kaichen Zhang*,
      
      
      
      
      
      
      
      
      <u>Bo Li*</u>,
      
      
      
      
      
      
      
      
      Peiyuan Zhang*,
      
      
      
      
      
      
      
      
      
      Fanyi Pu*,
      
      
      
      
      
      
      
      
      
      Joshua Adrian Cahyono*,
      
      
      
      
      
      
      
      
      
      Kairui Hu*,
      
      
      
      
      
      
      
      
      
      Shuai Liu*,
      
      
      
      
      
      
      
      
      
      Yuanhan Zhang*,
      
      
      
      
      
      
      
      
      
      Jingkang Yang*,
      
      
      
      
      
      
      
      
      
      Chunyuan Li*,
      
      
      
      
      
      
      
      
      
      Ziwei Liu*
      
      
      
      
      
    </span> 
        <span class="title" style="font-size: 16px;"> <a href="https://github.com/EvolvingLMMs-Lab/lmms-eval">LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models</a></span> 

        <span class="periodical">
      <nobr style="color:darkcyan; font-weight: 600;">
        
      </nobr>
      
      
      <em>North American Chapter of the Association for Computational Linguistics (NAACL), 2025,</em>
      
      <b style="color:darkred"><br />Open-source evaluation frameworks spanning text, image, video, and audio tasks with 2.4K GitHub stars; contributed core framework and major code.</b>
      
      <!-- 
        2024
       -->

      <nobr class="links" style="font-weight: 600;">
        
        
        
        
        
        
        
        
        
        
      </nobr>

    </span>  


        <!-- Hidden abstract block -->
        
    </div>
</div></li>
<li><div class="row">
    <div class="col-sm-1">
        [12]
    </div>

    <div id="liu2024llavanext" class="col-sm-11">
        
        <span class="author">
      
      
      
      
      
      Haotian Liu,
      
      
      
      
      
      
      
      
      
      Chunyuan Li,
      
      
      
      
      
      
      
      
      
      Yuheng Li,
      
      
      
      
      
      
      
      
      <u>Bo Li</u>,
      
      
      
      
      
      
      
      
      Yuanhan Zhang,
      
      
      
      
      
      
      
      
      
      Sheng Shen,
      
      
      
      
      
      
      
      
      
      Yong Jae Lee
      
      
      
      
      
    </span> 
        <span class="title" style="font-size: 16px;"> <a href="https://llava-vl.github.io/blog/2024-01-30-llava-next/">LLaVA-NeXT: Improved reasoning, OCR, and world knowledge</a></span> 

        <span class="periodical">
      <nobr style="color:darkcyan; font-weight: 600;">
        
      </nobr>
      
      
      <em>Technical Blog,</em>
      
      <b style="color:darkred"><br />First open models achieving GPT-4V-level performance, trained for 24 hours on 32 A100 GPUs. Proposed the idea that
massive evaluation leads to better models.</b>
      
      <!-- 
        2024
       -->

      <nobr class="links" style="font-weight: 600;">
        
        
        
        
        
        
        
        
        [<a href="https://github.com/LLaVA-VL/LLaVA-NeXT" target="_blank">code</a>]
        
        
        
      </nobr>

    </span>  


        <!-- Hidden abstract block -->
        
    </div>
</div></li>
<li><div class="row">
    <div class="col-sm-1">
        [11]
    </div>

    <div id="li2023mimic" class="col-sm-11">
        
        <span class="author">
      
      
      
      
      <u>Bo Li*</u>,
      
      
      
      
      
      
      
      
      Yuanhan Zhang*,
      
      
      
      
      
      
      
      
      
      Liangyu Chen,
      
      
      
      
      
      
      
      
      
      Jinghao Wang,
      
      
      
      
      
      
      
      
      
      Fanyi Pu,
      
      
      
      
      
      
      
      
      
      Jingkang Yang,
      
      
      
      
      
      
      
      
      
      Chunyuan Li,
      
      
      
      
      
      
      
      
      
      Ziwei Liu
      
      
      
      
      
    </span> 
        <span class="title" style="font-size: 16px;"> <a href="https://arxiv.org/abs/2306.05425">MIMIC-IT: Multi-modal In-Context Instruction Tuning</a></span> 

        <span class="periodical">
      <nobr style="color:darkcyan; font-weight: 600;">
        
      </nobr>
      
      
      <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2025,</em>
      
      <b style="color:darkred"><br />Very early (2023-10) experiment on a vision-language-agent (VLA) model with RLHF; proposed the idea and drafted the training code.</b>
      
      <!-- 
        2023
       -->

      <nobr class="links" style="font-weight: 600;">
        
        
        
        
        
        
        
        
        [<a href="https://github.com/Luodian/Otter" target="_blank">code</a>]
        
        
        
      </nobr>

    </span>  


        <!-- Hidden abstract block -->
        
    </div>
</div></li>
<li><div class="row">
    <div class="col-sm-1">
        [10]
    </div>

    <div id="li2023genbench" class="col-sm-11">
        
        <span class="author">
      
      
      
      
      <u>Bo Li</u>,
      
      
      
      
      
      
      
      
      Haotian Liu,
      
      
      
      
      
      
      
      
      
      Liangyu Chen,
      
      
      
      
      
      
      
      
      
      Yong Jae Lee,
      
      
      
      
      
      
      
      
      
      Chunyuan Li,
      
      
      
      
      
      
      
      
      
      Ziwei Liu
      
      
      
      
      
    </span> 
        <span class="title" style="font-size: 16px;"> <a href="https://arxiv.org/abs/2307.13697">Benchmarking and Analyzing Generative Data for Visual Recognition</a></span> 

        <span class="periodical">
      <nobr style="color:darkcyan; font-weight: 600;">
        
      </nobr>
      
      
      <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2025,</em>
      
      <b style="color:darkred"><br />First batch (2023-04)of open-source multimodal models.</b>
      
      <!-- 
        2023
       -->

      <nobr class="links" style="font-weight: 600;">
        
        
        
        
        
        
        
        
        [<a href="https://github.com/Luodian/GenBench" target="_blank">code</a>]
        
        
        
      </nobr>

    </span>  


        <!-- Hidden abstract block -->
        
    </div>
</div></li>
<li><div class="row">
    <div class="col-sm-1">
        [8]
    </div>

    <div id="li2022sparsf" class="col-sm-11">
        
        <span class="author">
      
      
      
      
      <u>Bo Li*</u>,
      
      
      
      
      
      
      
      
      Yifei Shen*,
      
      
      
      
      
      
      
      
      
      Jingkang Yang,
      
      
      
      
      
      
      
      
      
      Yezhen Wang,
      
      
      
      
      
      
      
      
      
      Jiawei Ren,
      
      
      
      
      
      
      
      
      
      Tong Che,
      
      
      
      
      
      
      
      
      
      Jun Zhang,
      
      
      
      
      
      
      
      
      
      Ziwei Liu
      
      
      
      
      
    </span> 
        <span class="title" style="font-size: 16px;"> <a href="https://arxiv.org/abs/2206.04046">Sparse Mixture-of-Experts are Domain Generalizable Learners</a></span> 

        <span class="periodical">
      <nobr style="color:darkcyan; font-weight: 600;">
        
        
          ICLR 2023 (Oral),
          
          
      </nobr>
      
      
      <em>In International Conference on Representation Learning 2023,</em>
      
      <b style="color:darkred"><br />First batch (2022-05) theoretical analysis of the mixture-of-experts architecture from a generalization perspective.</b>
      
      <!-- 
        2022
       -->

      <nobr class="links" style="font-weight: 600;">
        
        
        
        
        
        
        
        
        [<a href="https://github.com/Luodian/Generalizable-Mixture-of-Experts" target="_blank">code</a>]
        
        
        
      </nobr>

    </span> 
        <span class="periodical">
    <em>Short version in NeurIPS 2022 Workshop on Distribution Shift.</em>  
    </span>  


        <!-- Hidden abstract block -->
        
    </div>
</div></li>
<li><div class="row">
    <div class="col-sm-1">
        [9]
    </div>

    <div id="li2022sparse" class="col-sm-11">
        
        <span class="author">
      
      
      
      
      
      Liangyu Chen*,
      
      
      
      
      
      
      
      
      <u>Bo Li*</u>,
      
      
      
      
      
      
      
      
      Sheng Shen,
      
      
      
      
      
      
      
      
      
      Jingkang Yang,
      
      
      
      
      
      
      
      
      
      Chunyuan Li,
      
      
      
      
      
      
      
      
      
      Kurt Keutzer,
      
      
      
      
      
      
      
      
      
      Trevor Darrell,
      
      
      
      
      
      
      
      
      
      Ziwei Liu
      
      
      
      
      
    </span> 
        <span class="title" style="font-size: 16px;"> <a href="https://openreview.net/forum?id=kdHpWogtX6Y">Coordinating Multiple Vision-Language Models for Visual Reasoning</a></span> 

        <span class="periodical">
      <nobr style="color:darkcyan; font-weight: 600;">
        
        
          NeurIPS 2023,
          
          
      </nobr>
      
      
      <em>In Conference on Neural Information Processing Systems,</em>
      
      <b style="color:darkred"><br />Very early (2022-12) experiment using synthetic data for visual recognition.</b>
      
      <!-- 
        2022
       -->

      <nobr class="links" style="font-weight: 600;">
        
        
        
        
        
        
        
        
        
        
      </nobr>

    </span> 
        <span class="periodical">
    <em>Short version in ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models (ME-FoMo).</em>  
    </span>  


        <!-- Hidden abstract block -->
        
    </div>
</div></li>
<li><div class="row">
    <div class="col-sm-1">
        [6]
    </div>

    <div id="li2022invariant" class="col-sm-11">
        
        <span class="author">
      
      
      
      
      <u>Bo Li</u>,
      
      
      
      
      
      
      
      
      Yifei Shen,
      
      
      
      
      
      
      
      
      
      Yezhen Wang,
      
      
      
      
      
      
      
      
      
      Wenzhen Zhu,
      
      
      
      
      
      
      
      
      
      Dongsheng Li,
      
      
      
      
      
      
      
      
      
      Kurt Keutzer,
      
      
      
      
      
      
      
      
      
      Han Zhao
      
      
      
      
      
    </span> 
        <span class="title" style="font-size: 16px;"> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20703">Invariant information bottleneck for domain generalization</a></span> 

        <span class="periodical">
      <nobr style="color:darkcyan; font-weight: 600;">
        
        
          AAAI 2022,
          
          
      </nobr>
      
      
      <em>In Proceedings of the AAAI Conference on Artificial Intelligence.</em>
      
      
      <!-- 
        2021
       -->

      <nobr class="links" style="font-weight: 600;">
        
        
        
        
        
        
        
        
        [<a href="https://github.com/Luodian/IIB" target="_blank">code</a>]
        
        
        
      </nobr>

    </span>  


        <!-- Hidden abstract block -->
        
    </div>
</div></li>
<li><div class="row">
    <div class="col-sm-1">
        [5]
    </div>

    <div id="Wang_2021_ICCV" class="col-sm-11">
        
        <span class="author">
      
      
      
      
      
      Yezhen Wang,
      
      
      
      
      
      
      
      
      <u>Bo Li</u>,
      
      
      
      
      
      
      
      
      Tong Che,
      
      
      
      
      
      
      
      
      
      Kaiyang Zhou,
      
      
      
      
      
      
      
      
      
      Ziwei Liu,
      
      
      
      
      
      
      
      
      
      Dongsheng Li
      
      
      
      
      
    </span> 
        <span class="title" style="font-size: 16px;"> <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Energy-Based_Open-World_Uncertainty_Modeling_for_Confidence_Calibration_ICCV_2021_paper.html">Energy-Based Open-World Uncertainty Modeling for Confidence Calibration</a></span> 

        <span class="periodical">
      <nobr style="color:darkcyan; font-weight: 600;">
        
        
          ICCV 2021,
          
          
      </nobr>
      
      
      <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV).</em>
      
      
      <!-- 
        2021
       -->

      <nobr class="links" style="font-weight: 600;">
        
        
        
        
        
        
        
        
        [<a href="https://github.com/BIGKnight/Energy-Based-Open-World-Uncertainty-Modeling-for-Confidence-Calibration" target="_blank">code</a>]
        
        
        
      </nobr>

    </span>  


        <!-- Hidden abstract block -->
        
    </div>
</div></li>
<li><div class="row">
    <div class="col-sm-1">
        [4]
    </div>

    <div id="li2021learning" class="col-sm-11">
        
        <span class="author">
      
      
      
      
      <u>Bo Li</u>,
      
      
      
      
      
      
      
      
      Yezhen Wang,
      
      
      
      
      
      
      
      
      
      Shanghang Zhang,
      
      
      
      
      
      
      
      
      
      Dongsheng Li,
      
      
      
      
      
      
      
      
      
      Kurt Keutzer,
      
      
      
      
      
      
      
      
      
      Trevor Darrell,
      
      
      
      
      
      
      
      
      
      Han Zhao
      
      
      
      
      
    </span> 
        <span class="title" style="font-size: 16px;"> <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Learning_Invariant_Representations_and_Risks_for_Semi-Supervised_Domain_Adaptation_CVPR_2021_paper.pdf">Learning invariant representations and risks for semi-supervised domain adaptation</a></span> 

        <span class="periodical">
      <nobr style="color:darkcyan; font-weight: 600;">
        
        
          CVPR 2021,
          
          
      </nobr>
      
      
      <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.</em>
      
      
      <!-- 
        2021
       -->

      <nobr class="links" style="font-weight: 600;">
        
        
        
        
        
        
        
        
        [<a href="https://github.com/Luodian/Learning-Invariant-Representations-and-Risks" target="_blank">code</a>]
        
        
        
      </nobr>

    </span>  


        <!-- Hidden abstract block -->
        
    </div>
</div></li>
<li><div class="row">
    <div class="col-sm-1">
        [3]
    </div>

    <div id="zhao2021madan" class="col-sm-11">
        
        <span class="author">
      
      
      
      
      
      Sicheng Zhao,
      
      
      
      
      
      
      
      
      <u>Bo Li</u>,
      
      
      
      
      
      
      
      
      Pengfei Xu,
      
      
      
      
      
      
      
      
      
      Xiangyu Yue,
      
      
      
      
      
      
      
      
      
      Guiguang Ding,
      
      
      
      
      
      
      
      
      
      Kurt Keutzer
      
      
      
      
      
    </span> 
        <span class="title" style="font-size: 16px;"> <a href="https://link.springer.com/article/10.1007/s11263-021-01479-3">MADAN: multi-source adversarial domain aggregation network for domain adaptation</a></span> 

        <span class="periodical">
      <nobr style="color:darkcyan; font-weight: 600;">
        
        
          IJCV 2021,
          
          
      </nobr>
      
      
      <em>International Journal of Computer Vision.</em>
      
      
      <!-- 
        2021
       -->

      <nobr class="links" style="font-weight: 600;">
        
        
        
        
        
        
        
        
        
        
      </nobr>

    </span>  


        <!-- Hidden abstract block -->
        
    </div>
</div></li>
<li><div class="row">
    <div class="col-sm-1">
        [2]
    </div>

    <div id="li2020rethinking" class="col-sm-11">
        
        <span class="author">
      
      
      
      
      <u>Bo Li</u>,
      
      
      
      
      
      
      
      
      Yezhen Wang,
      
      
      
      
      
      
      
      
      
      Tong Che,
      
      
      
      
      
      
      
      
      
      Shanghang Zhang,
      
      
      
      
      
      
      
      
      
      Yoshua Bengio,
      
      
      
      
      
      
      
      
      
      Kurt Keutzer
      
      
      
      
      
    </span> 
        <span class="title" style="font-size: 16px;"> <a href="https://arxiv.org/abs/2006.13352">Rethinking distributional matching based domain adaptation</a></span> 

        <span class="periodical">
      <nobr style="color:darkcyan; font-weight: 600;">
        
      </nobr>
      
      
      <em>arXiv preprint arXiv:2006.13352.</em>
      
      
      <!-- 
        2020
       -->

      <nobr class="links" style="font-weight: 600;">
        
        
        
        
        
        
        
        
        
        
      </nobr>

    </span>  


        <!-- Hidden abstract block -->
        
    </div>
</div></li>
<li><div class="row">
    <div class="col-sm-1">
        [1]
    </div>

    <div id="zhao2019multi" class="col-sm-11">
        
        <span class="author">
      
      
      
      
      
      Sicheng Zhao*,
      
      
      
      
      
      
      
      
      <u>Bo Li*</u>,
      
      
      
      
      
      
      
      
      Xiangyu Yue*,
      
      
      
      
      
      
      
      
      
      Yang Gu,
      
      
      
      
      
      
      
      
      
      Pengfei Xu,
      
      
      
      
      
      
      
      
      
      Runbo Hu,
      
      
      
      
      
      
      
      
      
      Hua Chai,
      
      
      
      
      
      
      
      
      
      Kurt Keutzer
      
      
      
      
      
    </span> 
        <span class="title" style="font-size: 16px;"> <a href="https://proceedings.neurips.cc/paper/2019/hash/db9ad56c71619aeed9723314d1456037-Abstract.html">Multi-source domain adaptation for semantic segmentation</a></span> 

        <span class="periodical">
      <nobr style="color:darkcyan; font-weight: 600;">
        
        
          NeurIPS 2019,
          
          
      </nobr>
      
      
      <em>In Neural Information Processing Systems.</em>
      
      
      <!-- 
        2019
       -->

      <nobr class="links" style="font-weight: 600;">
        
        
        
        
        
        
        
        
        [<a href="https://github.com/Luodian/MADAN" target="_blank">code</a>]
        
        
        
      </nobr>

    </span>  


        <!-- Hidden abstract block -->
        
    </div>
</div></li></ol>

</div>

<h5 id="experiences">Experiences</h5>
<p>I have been fortunately collaborating and doing research at/with</p>

<ul>
  <li>
    <p>Dec. 2023 - Aug. 2024: <a href="https://team.doubao.com/en/">Bytedance Seed Research, Singapore</a></p>

    <p>Supervised by Dr. <a href="https://chunyuan.li/">Chunyuan Li</a>. Dedicated in building open-source multimodal models.</p>
  </li>
  <li>
    <p>Dec. 2022 - Aug. 2023: <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-redmond/">Microsoft Research, Redmond</a></p>

    <p>Supervised by Dr. <a href="https://chunyuan.li/">Chunyuan Li</a>. Great experience on multimodal learning, learned a lot from great teammate <a href="https://hliu.cc/">Haotian Liu</a> and later participated in the LLaVA Journey.</p>
  </li>
  <li>
    <p>Sep. 2020 - Dec. 2021: <a href="https://www.microsoft.com/en-us/research/group/shanghai-ai-ml-group/">Microsoft Research, Shanghai</a></p>

    <p>Supervised by Dr. <a href="http://recmind.cn/">Dongsheng Li</a> in the beautiful and relaxing WestBud office, with chill and smart colleagues.</p>
  </li>
  <li>
    <p>Oct. 2019 - Aug. 2020 (remote till May 2021): <a href="https://bair.berkeley.edu/">Berkeley AI Research, CA, USA</a></p>

    <p>Supervised by Prof. <a href="https://people.eecs.berkeley.edu/~keutzer/">Kurt Keutzer</a>  and Prof. <a href="https://sites.google.com/view/schzhao">Sicheng Zhao</a>, Prof. <a href="https://www.ie.cuhk.edu.hk/people/xyyue.shtml">Xiangyu Yue</a>, Prof. <a href="https://www.shanghangzhang.com/">Shanghang Zhang</a> and Dr. <a href="https://people.eecs.berkeley.edu/~cjrd/">Colorado Reed</a>. Enjoy the weather and front-tier research atmosphere. Go Cal and Roll on your Golden Bears!</p>
  </li>
  <li>
    <p>Jan 2020 - Nov 2022: <a href="https://nvr-avg.github.io/author/gerry-che/">Dr. Tong Che</a>, MILA/Nvidia Research</p>

    <p>Great appreciation on guiding me to explore many fascinating ML topics.</p>
  </li>
  <li>
    <p>May 2020 - Dec. 2021: <a href="https://hanzhaoml.github.io/">Prof. Han Zhao</a>, UIUC</p>

    <p>Learn to write a paper with machine learning taste.</p>
  </li>
  <li>
    <p>May 2018 - Oct. 2019: <a href="https://www.didiglobal.com/science/ailabs">DiDi Visual Perception Team, Beijing</a></p>

    <p>First internship and two papers there.</p>
  </li>
</ul>

<hr />
<h5 id="professional-services">Professional Services</h5>
<ul>
  <li><strong>Talk/Technical Sharing</strong>:
    <ul>
      <li>Multimodal Models, LMMs-Lab Projects@Jump Trading (2025), Hosted by Weifeng Liu.</li>
      <li>Guest Lecture: Multimodal Models@UMich, UM EECS 542: Advanced Topics in Computer Vision, Hosted by <a href="https://web.eecs.umich.edu/~stellayu/index.html">Stella X. Yu</a></li>
      <li>Multimodal Models, LMMs-Lab Projects@TwelveLabs (2024), Hosted by <a href="https://jameskle.com/">James Le</a></li>
      <li>Multimodal Models, LMMs-Lab Projects@Tiktok (2024)</li>
      <li>Otter &amp; MIMICIT@Alibaba, Damo Academy, Hosted by <a href="https://lidongbing.github.io/">Dr. Lidong Bing</a>, Sep. 2023.</li>
      <li>Otter &amp; MIMICIT@HITSZ, Hosted by <a href="https://rshaojimmy.github.io/">Prof. Rui Shao</a>, Jul. 2023.</li>
    </ul>
  </li>
</ul>

<hr />
<ul>
  <li><strong><a href="https://www.mmlab-ntu.com/index.html">Slab@NTU</a></strong>: Cluster Adminstrator (70+ users, 400+ GPUs)</li>
</ul>

<hr />

<ul>
  <li><strong><a href="https://theaitalks.org/">The AI Talk</a></strong>: Organizer</li>
</ul>

<hr />

<ul>
  <li>
    <p><strong>Conference Reviewer / Program Committee</strong>:</p>

    <ul>
      <li>
        <p>ICCV (2021,2023), NeurIPS (2022), BMVC (2023), AAAI (2023), CVPR (2022,2023), AISTATS (2023), ICML (2023).</p>
      </li>
      <li>
        <p>Workshop: ICLR 2023 (DG)</p>
      </li>
    </ul>
  </li>
</ul>

<hr />

<ul>
  <li>
    <p><strong>Journal Reviewer</strong>:</p>

    <ul>
      <li>Pattern Recognition (PR)</li>
      <li>Transactions on Multimedia (TMM)</li>
      <li>Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</li>
      <li>International Journal of Computer Vision (IJCV)</li>
    </ul>
  </li>
</ul>

<!-- **Workshop organization**:
- [NeurIPS 2019 Workshop on Information Theory and Machine Learning](https://sites.google.com/view/itml19/home) (chair)
- [DALI 2018 Workshop on Generative Models and Reinforcement Learning](http://dalimeeting.org/dali2018//program) (chair) -->

<hr />

<p><strong>Acknowledgements</strong>: this website builds on <a href="https://github.com/alshedivat/al-folio">al-folio</a> and <a href="https://github.com/jiamings/tsong.me">Jiaming Song</a>.</p>

      </div>
        
    </div>

    <!-- 
      <div class="news">
  <h2>news</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
      </table>
    </div>
  
</div>

     -->

    <!-- 
    <div class="social">
      <div class="contact-note">libo0013@ntu.edu.sg
</div>
    </div>
     -->
  </article>

</div>

    </div>

    <!-- Footer -->

    <!-- 
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025 Brian (Bo) Li.
    
    
  </div>
</footer>
 -->


  </body>

  <!-- Load Core and Bootstrap JS -->
<script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.0/umd/popper.min.js" integrity="sha256-OH05DFHUWzr725HmuHo3pnuvUUn+TJuj8/Qz9xytFEw=" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.17.0/js/mdb.min.js"  integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />


<!-- Load KaTeX -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" integrity="sha256-V8SV2MO1FUb63Bwht5Wx9x6PVHNa02gv8BgH/uH3ung=" crossorigin="anonymous" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js" integrity="sha256-F/Xda58SPdcUCr+xhSGz9MA2zQBPb0ASEYKohl8UCHc=" crossorigin="anonymous"></script>
<script src="/assets/js/katex.js"></script>



<!-- Load Mansory & imagesLoaded -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="" crossorigin="anonymous"></script>
<script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>

<!-- Project Cards Layout -->
<script type="text/javascript">
  // Init Masonry
  var $grid = $('.grid').masonry({
    gutter: 10,
    horizontalOrder: true,
    itemSelector: '.grid-item',
  });
  // layout Masonry after each image loads
  $grid.imagesLoaded().progress( function() {
    $grid.masonry('layout');
  });
</script>







</html>
