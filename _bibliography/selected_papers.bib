@article{li2025aero,
  	title={Aero-1-Audio},
  	author={Li*, Bo and Chen, Change Loy and Pu, Fanyi and Yang, Jingkang and Zhang*, Kaichen and Hu, Kairui and Thang*, Luu Minh and Trung*, Nguyen Quang and Cong*, Pham Ba and Liu, Shuai and Wang*, Yezhen and Liu, Ziwei},
    url={https://www.lmms-lab.com/posts/aero_audio/},
  	journal={Technical Blog},
  	year={2025},
  	month={May},
  	count={18},
    additional={<br>Open models for wide range of audio tasks, trained on only 50K hours data yet achieving excellent performance, suggesting smart data > massive training.},
}

@article{li2024llava,
  	title={LLaVA-OneVision: Easy Visual Task Transfer},
  	author={Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Li, Yanwei and Liu, Ziwei and Li, Chunyuan},
    url={https://arxiv.org/abs/2408.03326},
  	journal={Transactions on Machine Learning Research (TMLR), 2025},
  	year={2024},
  	month={August},
  	count={17},
    additional={<br>SOTA-level fully open models (models/data/code)
          achieving GPT-4o-level performance across 30+ image and video
          tasks. Led codebase, datacuration, and evaluation},
}

@article{li2024lmmseval,
    title={LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models},
    author={Zhang*, Kaichen and Li*, Bo and Zhang*, Peiyuan and Pu*, Fanyi and Cahyono*, Joshua Adrian and Hu*, Kairui and Liu*, Shuai and Zhang*, Yuanhan and Yang*, Jingkang and Li*, Chunyuan and Liu*, Ziwei},
    url={https://github.com/EvolvingLMMs-Lab/lmms-eval},
    journal={North American Chapter of the Association for Computational Linguistics (NAACL), 2025},
    year={2024},
    month={July},
    count={16},
    additional={<br>Open-source evaluation frameworks spanning text, image, video, and audio tasks with 2.4K GitHub stars; contributed core framework and major code},
}

@article{liu2024llavanext,
    title={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},
    url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},
    journal={Technical Blog},
    code={https://github.com/LLaVA-VL/LLaVA-NeXT},
    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
    month={January},
    year={2024},
    count={12},
    additional={<br>First open models achieving GPT-4V-level performance, trained for 24 hours on 32 A100 GPUs. Proposed the idea that
massive evaluation leads to better models}
}

@article{li2023mimic,
  title={MIMIC-IT: Multi-modal In-Context Instruction Tuning},
  author={Li*, Bo and Zhang*, Yuanhan and Chen, Liangyu and Wang, Jinghao and Pu, Fanyi and Yang, Jingkang and Li, Chunyuan and Liu, Ziwei},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2025},
  url={https://arxiv.org/abs/2306.05425},
  code={https://github.com/Luodian/Otter},
  month        = jun,
  year         = 2023,
  count={11},
  additional={<br>Very early (2023-10) experiment on a vision-language-agent (VLA) model with RLHF; proposed the idea and drafted the training code}
}

@article{li2023genbench,
  title={Benchmarking and Analyzing Generative Data for Visual Recognition},
  author={Li, Bo and Liu, Haotian and Chen, Liangyu and Lee, Yong Jae and Li, Chunyuan and Liu, Ziwei},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2025},
  url={https://arxiv.org/abs/2307.13697},
  code={https://github.com/Luodian/GenBench},
  month        = apr,
  year         = 2023,
  count={10},
  additional={<br>First batch (2023-04)of open-source multimodal models}
}

@inproceedings{li2022sparse,
title={Coordinating Multiple Vision-Language Models for Visual Reasoning},
author={Chen*, Liangyu and Li*, Bo and Shen, Sheng and Yang, Jingkang and Li, Chunyuan and Keutzer, Kurt and Darrell, Trevor and Liu, Ziwei},
url={https://openreview.net/forum?id=kdHpWogtX6Y},
booktitle={Conference on Neural Information Processing Systems},
abbr={NeurIPS 2023},
workshop={ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models (ME-FoMo)},
month        = oct,
year         = 2022,
count={9},
additional={<br>Very early (2022-12) experiment using synthetic data for visual recognition}
}

@inproceedings{li2022sparse,
title={Sparse Mixture-of-Experts are Domain Generalizable Learners},
author={Li*, Bo and Shen*, Yifei and Yang, Jingkang and Wang, Yezhen and Ren, Jiawei and Che, Tong and Zhang, Jun and Liu, Ziwei},
booktitle={International Conference on Representation Learning 2023},
abbr={ICLR 2023 (Oral)},
url={https://arxiv.org/abs/2206.04046},
code={https://github.com/Luodian/Generalizable-Mixture-of-Experts},
workshop={NeurIPS 2022 Workshop on Distribution Shift},
month        = oct,
year         = 2022,
count={8},
additional={<br>First batch (2022-05) theoretical analysis of the mixture-of-experts architecture from a generalization perspective}
}


@inproceedings{li2022invariant,
title={Invariant information bottleneck for domain generalization},
author={Li, Bo and Shen, Yifei and Wang, Yezhen and Zhu, Wenzhen and Li, Dongsheng and Keutzer, Kurt and Zhao, Han},
url={https://ojs.aaai.org/index.php/AAAI/article/view/20703},
code={https://github.com/Luodian/IIB},
booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
volume={36},
number={7},
pages={7399--7407},
month        = oct,
year         = 2021,
abbr={AAAI 2022},
count={6}
}

@InProceedings{Wang_2021_ICCV,
    author    = {Wang, Yezhen and Li, Bo and Che, Tong and Zhou, Kaiyang and Liu, Ziwei and Li, Dongsheng},
    title     = {Energy-Based Open-World Uncertainty Modeling for Confidence Calibration},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    abbr={ICCV 2021},
    url={https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Energy-Based_Open-World_Uncertainty_Modeling_for_Confidence_Calibration_ICCV_2021_paper.html},
    code={https://github.com/BIGKnight/Energy-Based-Open-World-Uncertainty-Modeling-for-Confidence-Calibration},
    month     = {October},
    year      = {2021},
    pages     = {9302-9311},
    count={5}
}

@inproceedings{li2021learning,
  title={Learning invariant representations and risks for semi-supervised domain adaptation},
  author={Li, Bo and Wang, Yezhen and Zhang, Shanghang and Li, Dongsheng and Keutzer, Kurt and Darrell, Trevor and Zhao, Han},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1104--1113},
  abbr={CVPR 2021},
  code={https://github.com/Luodian/Learning-Invariant-Representations-and-Risks},
  url={https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Learning_Invariant_Representations_and_Risks_for_Semi-Supervised_Domain_Adaptation_CVPR_2021_paper.pdf},
  year={2021},
  count={4}
}

@article{zhao2021madan,
  title={MADAN: multi-source adversarial domain aggregation network for domain adaptation},
  author={Zhao, Sicheng and Li, Bo and Xu, Pengfei and Yue, Xiangyu and Ding, Guiguang and Keutzer, Kurt},
  journal={International Journal of Computer Vision},
  abbr={IJCV 2021},
  url={https://link.springer.com/article/10.1007/s11263-021-01479-3},
  volume={129},
  number={8},
  pages={2399--2424},
  year={2021},
  publisher={Springer},
  count={3}
}

@article{li2020rethinking,
  title={Rethinking distributional matching based domain adaptation},
  author={Li, Bo and Wang, Yezhen and Che, Tong and Zhang, Shanghang and Bengio, Yoshua and Keutzer, Kurt},
  journal={arXiv preprint arXiv:2006.13352},
  url={https://arxiv.org/abs/2006.13352},
  year={2020},
  count={2}
}

@inproceedings{zhao2019multi,
  title={Multi-source domain adaptation for semantic segmentation},
  author={Zhao*, Sicheng and Li*, Bo and Yue*, Xiangyu and Gu, Yang and Xu, Pengfei and Hu, Runbo and Chai, Hua and Keutzer, Kurt},
  month={December},
  booktitle={Neural Information Processing Systems},
  abbr={NeurIPS 2019},
  code={https://github.com/Luodian/MADAN},
  url={https://proceedings.neurips.cc/paper/2019/hash/db9ad56c71619aeed9723314d1456037-Abstract.html},
  year={2019},
  count={1}
}